{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import torch\n",
    "import sklearn\n",
    "import re\n",
    "import pyparsing\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the provided file while removing the functional labels\n",
    "\n",
    "file_name = \"sequoia-corpus+fct.mrg_strict\"\n",
    "\n",
    "data = []\n",
    "with open(\"./\" + file_name, \"r\") as f:\n",
    "    for sentence in f:\n",
    "        sentence = sentence.strip()\n",
    "        #sentence = filter_labels(sentence) ,can be done later and more easily   \n",
    "        data.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( (SENT (NP-SUJ (DET Cette) (NC exposition)) (VN (CLO-A_OBJ nous) (V apprend)) (Ssub-OBJ (CS que) (PP-MOD (P dès) (NP (DET le) (ADJ XIIe) (NC siècle))) (PONCT ,) (PP-MOD (P à) (NP (NPP Dammarie-sur-Saulx))) (PONCT ,) (PP-MOD (P entre) (NP (ADJ autres) (NC sites))) (PONCT ,) (NP-SUJ (DET une) (NC industrie) (AP (ADJ métallurgique))) (VN (V existait))) (PONCT .)))\n"
     ]
    }
   ],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(expr):\n",
    "    def _helper(s):\n",
    "        items = []\n",
    "        word = []\n",
    "        for item in s:\n",
    "            \n",
    "            if word and item == ')': # Terminal token (aka. natural word)\n",
    "                word = ''.join(word).lower()\n",
    "                items.append(word)\n",
    "                word = []\n",
    "            \n",
    "            elif word and (item == '(' or item == ' '): # Tags\n",
    "                word = ''.join(word)\n",
    "                items.append(word)\n",
    "                word = []\n",
    "                \n",
    "            if item == '(':\n",
    "                result, closeparen = _helper(s)\n",
    "                if not closeparen:\n",
    "                    raise ValueError(\"bad expression -- unbalanced parentheses\")\n",
    "                items.append(result)\n",
    "            \n",
    "            elif item == ')':\n",
    "                return items, True\n",
    "            \n",
    "            elif item != ' ':\n",
    "                word.append(item)\n",
    "        \n",
    "        return items, False\n",
    "    return _helper(iter(expr))[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def untag(expr):\n",
    "    def _helper(s):\n",
    "        items = []\n",
    "        word = []\n",
    "        for item in s:\n",
    "            \n",
    "            if word and item == ')':\n",
    "                word = ''.join(word)\n",
    "                items.append(word)\n",
    "                word = []\n",
    "            \n",
    "            elif word and (item == '(' or item == ' '):\n",
    "                word = []\n",
    "                \n",
    "            if item == '(':\n",
    "                result, closeparen = _helper(s)\n",
    "                if not closeparen:\n",
    "                    raise ValueError(\"bad expression -- unbalanced parentheses\")\n",
    "                for elmnt in result:\n",
    "                    items.append(elmnt)\n",
    "                    \n",
    "            elif item == ')':\n",
    "                return items, True\n",
    "            \n",
    "            elif item != ' ':\n",
    "                word.append(item)\n",
    "        \n",
    "        return items, False\n",
    "    return _helper(iter(expr))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_untagged_sentences(sentences):\n",
    "    \"\"\"Creates file and variable containing untagged sentences\"\"\"\n",
    "    \n",
    "    result = []\n",
    "    with open(\"sentences.txt\", \"w\") as f:\n",
    "        for s in sentences:\n",
    "            parentheses_stack = []\n",
    "            tags_stack = []\n",
    "            s = untag(s)\n",
    "            s = ' '.join(s)\n",
    "            f.write(s)\n",
    "            result.append(s)\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "untagged_sentences = get_untagged_sentences(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_tags = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tags(s, d, tag_set):\n",
    "    \"\"\"Adds the current node's parent/child relation count to the probability dictionary\"\"\"\n",
    "    tag_name = s[0].split(\"-\")[0] # Remove any hiphen on tags\n",
    "    if len(s) == 2 and isinstance(s[1], str):\n",
    "        return\n",
    "    root_tags.add(tag_name)\n",
    "    childs = s[1:]\n",
    "    child_tags = None\n",
    "    \n",
    "    for c in childs:\n",
    "        child_tag_name = c[0].split(\"-\")[0]\n",
    "        tag_set.add(child_tag_name)\n",
    "        if child_tags is None:\n",
    "            child_tags =  child_tag_name\n",
    "        else:\n",
    "            child_tags = ','.join([child_tags, child_tag_name])\n",
    "        get_tags(c, d, tag_set)\n",
    "    d[tag_name] = d.get(tag_name, {})\n",
    "    d[tag_name][child_tags] = d[tag_name].get(child_tags, 0) + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pcfg(sentences, tag_set):\n",
    "    \"\"\"Builds the PCFG using the tagged sentences and populates a tag_set\"\"\"\n",
    "\n",
    "    prob_dict = dict()\n",
    "\n",
    "    for s in sentences:\n",
    "        parentheses_stack = []\n",
    "        tags_stack = []\n",
    "        s = parse(s)\n",
    "        get_tags(s, prob_dict, tag_set)\n",
    "\n",
    "    for k1, d in prob_dict.items():\n",
    "        tag_set.add(k1)\n",
    "        tot_count = max(sum(d.values()), 1)\n",
    "        for k2, v in d.items():\n",
    "            d[k2] = v/tot_count\n",
    "\n",
    "    return prob_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "6\n",
      "15\n",
      "{'NP', 'CS', 'V', 'NC', 'ADJ', 'AP', 'NPP', 'PP', 'P', 'VN', 'Ssub', 'PONCT', 'CLO', 'SENT', 'DET'}\n"
     ]
    }
   ],
   "source": [
    "tag_set = set()\n",
    "\n",
    "pcfg = build_pcfg(data, tag_set)\n",
    "print(len(pcfg))\n",
    "print(len(root_tags))\n",
    "print(len(tag_set))\n",
    "print(tag_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NP': {'DET,NC': 0.2,\n",
       "  'DET,ADJ,NC': 0.2,\n",
       "  'NPP': 0.2,\n",
       "  'ADJ,NC': 0.2,\n",
       "  'DET,NC,AP': 0.2},\n",
       " 'VN': {'CLO,V': 0.5, 'V': 0.5},\n",
       " 'PP': {'P,NP': 1.0},\n",
       " 'AP': {'ADJ': 1.0},\n",
       " 'Ssub': {'CS,PP,PONCT,PP,PONCT,PP,PONCT,NP,VN': 1.0},\n",
       " 'SENT': {'NP,VN,Ssub,PONCT': 1.0}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_counts(s, d):\n",
    "    \"\"\"Adds the word count to the lexicon dictionary\"\"\"\n",
    "    tag_name = s[0].split(\"-\")[0] # Do not remove hyphens\n",
    "    if len(s) == 2 and isinstance(s[1], str):\n",
    "        word = s[1].lower()\n",
    "        d[word] = d.get(word, {})\n",
    "        d[word][tag_name] = d[word].get(tag_name, 0) + 1\n",
    "        return\n",
    "    childs = s[1:]\n",
    "    for c in childs:\n",
    "        get_word_counts(c, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prob_lexicon(sentences, tag_set):\n",
    "    \"\"\"Builds a probabilistic lexicon using the tagged sentences\"\"\"\n",
    "    \n",
    "    lexicon_dict = dict()\n",
    "    \n",
    "    for s in sentences:\n",
    "        parentheses_stack = []\n",
    "        tags_stack = []\n",
    "        s = parse(s)\n",
    "        get_word_counts(s, lexicon_dict)\n",
    "    \n",
    "    for _, d in lexicon_dict.items():\n",
    "        tot_count = max(sum(d.values()), 1)\n",
    "        for k2, v in d.items():\n",
    "            tag_set.add(k2)\n",
    "            d[k2] = v/tot_count\n",
    "    \n",
    "    return lexicon_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cette': {'DET': 1.0}, 'exposition': {'NC': 1.0}, 'nous': {'CLO': 1.0}, 'apprend': {'V': 1.0}, 'que': {'CS': 1.0}, 'dès': {'P': 1.0}, 'le': {'DET': 1.0}, 'xiie': {'ADJ': 1.0}, 'siècle': {'NC': 1.0}, ',': {'PONCT': 1.0}, 'à': {'P': 1.0}, 'dammarie-sur-saulx': {'NPP': 1.0}, 'entre': {'P': 1.0}, 'autres': {'ADJ': 1.0}, 'sites': {'NC': 1.0}, 'une': {'DET': 1.0}, 'industrie': {'NC': 1.0}, 'métallurgique': {'ADJ': 1.0}, 'existait': {'V': 1.0}, '.': {'PONCT': 1.0}}\n"
     ]
    }
   ],
   "source": [
    "lexicon = build_prob_lexicon(data, tag_set)\n",
    "print(lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tag_counts(s, d):\n",
    "    \"\"\"Adds the tag count to the tag dictionary\"\"\"\n",
    "    tag_name = s[0] # Do not remove hyphens\n",
    "    if len(s) == 2 and isinstance(s[1], str):\n",
    "        d[tag_name] = d.get(tag_name, 0) + 1 \n",
    "        return\n",
    "    childs = s[1:]\n",
    "    for c in childs:\n",
    "        get_tag_counts(c, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tag_frequencies(sentences):\n",
    "    \"\"\"Builds a tag frequency list\"\"\"\n",
    "    \n",
    "    tag_dict = dict()\n",
    "    \n",
    "    for s in sentences:\n",
    "        parentheses_stack = []\n",
    "        tags_stack = []\n",
    "        s = parse(s)\n",
    "        get_tag_counts(s, tag_dict)\n",
    "        \n",
    "    tot_count = max(sum(tag_dict.values()), 1)\n",
    "    for k1, v in tag_dict.items():\n",
    "        tag_dict[k1] = v/tot_count\n",
    "    \n",
    "    return tag_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DET': 0.13636363636363635, 'NC': 0.18181818181818182, 'CLO-A_OBJ': 0.045454545454545456, 'V': 0.09090909090909091, 'CS': 0.045454545454545456, 'P': 0.13636363636363635, 'ADJ': 0.13636363636363635, 'PONCT': 0.18181818181818182, 'NPP': 0.045454545454545456}\n"
     ]
    }
   ],
   "source": [
    "tag_freq = build_tag_frequencies(data)\n",
    "print(tag_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unit(cfg, A, B, tag_set):\n",
    "    \"\"\"Replaces the Unit rules by Multiple Rules by chaining\"\"\"\n",
    "    \n",
    "    d = cfg[A]\n",
    "    copy_d = copy.deepcopy(cfg[A])\n",
    "    freq = copy_d[B]\n",
    "    copy_lexicon = copy.deepcopy(lexicon)\n",
    "    \n",
    "    # Create new rules for telescoping\n",
    "    if B not in cfg: # If preterminal tag\n",
    "        if A != \"SENT\":\n",
    "            new_key = A + \"&\" + B\n",
    "            tag_set.add(new_key)\n",
    "            for word, tags in copy_lexicon.items():\n",
    "                for tag, prob in tags.items():\n",
    "                    if tag == B:\n",
    "                        lexicon[word][new_key] = prob * freq\n",
    "            \n",
    "            for k1, prob in copy_d.items():\n",
    "                keys = k1.split(',')\n",
    "                if len(keys) == 2 and A == keys[1]:\n",
    "                    keys = ','.join([keys[0],new_key])\n",
    "                    cfg[k1].update({keys : prob * freq})\n",
    "    \n",
    "    else: # If not preterminal tag\n",
    "        for tags, prob in cfg[B].items():\n",
    "            if len(tags) == 2:\n",
    "                cfg[A][tags] = prob * freq \n",
    "        \n",
    "    del d[B]\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_multi(cfg, A, B, idx, count, tag_set):\n",
    "    \"\"\"Replaces the Multiple Rules with chains of Double or Unit Rules\"\"\"\n",
    "    tags = B.split(',')\n",
    "    n = len(tags)\n",
    "    d = cfg[A]\n",
    "    \n",
    "    new_tag = A + str(count)\n",
    "    count += 1\n",
    "    new_key = ','.join([tags[0], new_tag])\n",
    "    d[new_key] = copy.deepcopy(d[B])\n",
    "    del d[B]\n",
    "    \n",
    "    for i in range(1, n-2): \n",
    "        tag_set.add(new_tag)\n",
    "        current_key = new_tag\n",
    "        new_tag = A + str(count)\n",
    "        count += 1\n",
    "        cfg[current_key] =  {','.join([tags[i], new_tag]): d[new_key]}\n",
    "    \n",
    "    tag_set.add(new_tag)\n",
    "    cfg[new_tag] = {tags[-1] : d[new_key]}\n",
    "    \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chomskyfy(context_free_grammar, tag_set):\n",
    "    \"\"\"Binarisation of a CFG. Starts with removing Unit and then Multi Rules\"\"\"\n",
    "    cfg = copy.deepcopy(context_free_grammar)\n",
    "      \n",
    "    count = 0\n",
    "    copy_cfg = copy.deepcopy(cfg) # Avoid Dictionnary size change during iteration\n",
    "    for k1, d1 in copy_cfg.items():\n",
    "        copy_d1 = copy.deepcopy(d1) # Avoid Dictionnary size change during iteration\n",
    "        for i, (k2, prob) in enumerate(copy_d1.items()):\n",
    "            tags = k2.split(',')\n",
    "            if len(tags) > 2:\n",
    "                count = remove_multi(cfg, k1, k2, i, count, tag_set)\n",
    "    \n",
    "    return cfg\n",
    "\n",
    "    copy_cfg = copy.deepcopy(cfg) # Avoid Dictionnary size change during iteration\n",
    "    for k1, d1 in copy_cfg.items():\n",
    "        copy_d1 = copy.deepcopy(d1) # Avoid Dictionnary size change during iteration\n",
    "        for k2, prob in copy_d1.items():\n",
    "            tags = k2.split(',')\n",
    "            if len(tags) == 1 and tags[0] != k1: #Avoid loops \n",
    "                # Check that we have unit rules that we can remove\n",
    "                remove_unit(cfg, k1, tags[0], tag_set)\n",
    "    \n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NP': {'DET,NC': 0.2,\n",
       "  'NPP': 0.2,\n",
       "  'ADJ,NC': 0.2,\n",
       "  'DET,NP0': 0.2,\n",
       "  'DET,NP2': 0.2},\n",
       " 'VN': {'CLO,V': 0.5, 'V': 0.5},\n",
       " 'PP': {'P,NP': 1.0},\n",
       " 'AP': {'ADJ': 1.0},\n",
       " 'Ssub': {'CS,Ssub4': 1.0},\n",
       " 'SENT': {'NP,SENT12': 1.0},\n",
       " 'NP0': {'ADJ,NP1': 0.2},\n",
       " 'NP1': {'NC': 0.2},\n",
       " 'NP2': {'NC,NP3': 0.2},\n",
       " 'NP3': {'AP': 0.2},\n",
       " 'Ssub4': {'PP,Ssub5': 1.0},\n",
       " 'Ssub5': {'PONCT,Ssub6': 1.0},\n",
       " 'Ssub6': {'PP,Ssub7': 1.0},\n",
       " 'Ssub7': {'PONCT,Ssub8': 1.0},\n",
       " 'Ssub8': {'PP,Ssub9': 1.0},\n",
       " 'Ssub9': {'PONCT,Ssub10': 1.0},\n",
       " 'Ssub10': {'NP,Ssub11': 1.0},\n",
       " 'Ssub11': {'VN': 1.0},\n",
       " 'SENT12': {'VN,SENT13': 1.0},\n",
       " 'SENT13': {'Ssub,SENT14': 1.0},\n",
       " 'SENT14': {'PONCT': 1.0}}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_pcfg = chomskyfy(pcfg, tag_set)\n",
    "binary_pcfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CODE FROM THE POLYGLOT TUTORIAL ###\n",
    "\n",
    "from operator import itemgetter\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "words, embeddings = pickle.load(open('polyglot-fr.pkl', 'rb'), encoding='latin1')\n",
    "\n",
    "# Map words to indices and vice versa\n",
    "intersection_words = list()\n",
    "intersection_embeddings = list()\n",
    "\n",
    "for i, w in enumerate(words):\n",
    "    if w in lexicon:\n",
    "        intersection_words.append(w)\n",
    "        \n",
    "word_id = {w:i for (i, w) in enumerate(words)}\n",
    "id_word = dict(enumerate(embeddings))\n",
    "\n",
    "intersection_embeddings = np.array(intersection_embeddings)\n",
    "intersection_words = np.array(intersection_words)\n",
    "\n",
    "\n",
    "# Normalize digits by replacing them with #\n",
    "DIGITS = re.compile(\"[0-9]\", re.UNICODE)\n",
    "\n",
    "\n",
    "def case_normalizer(word, dictionary):\n",
    "    \"\"\" In case the word is not available in the vocabulary,\n",
    "     we can try multiple case normalizing procedure.\n",
    "     We consider the best substitute to be the one with the lowest index,\n",
    "     which is equivalent to the most frequent alternative.\"\"\"\n",
    "    w = word\n",
    "    lower = (dictionary.get(w.lower(), 1e12), w.lower())\n",
    "    upper = (dictionary.get(w.upper(), 1e12), w.upper())\n",
    "    title = (dictionary.get(w.title(), 1e12), w.title())\n",
    "    results = [lower, upper, title]\n",
    "    results.sort()\n",
    "    index, w = results[0]\n",
    "    if index != 1e12:\n",
    "        return w\n",
    "    return word\n",
    "\n",
    "\n",
    "def normalize(word, word_id):\n",
    "    \"\"\" Find the closest alternative in case the word is OOV.\"\"\"\n",
    "    if not word in word_id:\n",
    "        word = DIGITS.sub(\"#\", word)\n",
    "    if not word in word_id:\n",
    "        word = case_normalizer(word, word_id)\n",
    "\n",
    "    if not word in word_id:\n",
    "        return None\n",
    "    return word\n",
    "\n",
    "\n",
    "def l2_nearest(embeddings, words, word_index, k):\n",
    "    \"\"\"Sorts words according to their Euclidean distance.\n",
    "       To use cosine distance, embeddings has to be normalized so that their l2 norm is 1.\"\"\"\n",
    "\n",
    "    e1 = embeddings[word_index]\n",
    "    distances = []\n",
    "    for w2 in words:\n",
    "        e2 = embeddings[word_id[w2]]\n",
    "        distances.append(e1.dot(e2) / (np.linalg.norm(e1) * np.linalg.norm(e2)))\n",
    "    sorted_distances = sorted(enumerate(distances), key=itemgetter(1))\n",
    "    return zip(*sorted_distances[:k])\n",
    "\n",
    "\n",
    "def knn(word, words, embeddings, word_id, id_word, k=5):\n",
    "    word = normalize(word, word_id)\n",
    "    if not word:\n",
    "        print(\"OOV word\")\n",
    "        return None, None\n",
    "    word_index = word_id[word]\n",
    "    indices, distances = l2_nearest(embeddings, words, word_index, k)\n",
    "    neighbors = [id_word[idx] for idx in indices]\n",
    "    \n",
    "    return neighbors, distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenstein_dist(w1, w2):\n",
    "    \"\"\"Computes the levenstein distance between w1 and w2\"\"\"\n",
    "    \n",
    "    m, n  = len(w1),len(w2)\n",
    "    \n",
    "    result = np.zeros((m, n))\n",
    "    \n",
    "    for i in range(m):\n",
    "        result[i, 0] = i\n",
    "    for i in range(n):\n",
    "        result[0, i] = i\n",
    "        \n",
    "    for i, c1 in enumerate(w1):\n",
    "        for j, c2 in enumerate(w2):\n",
    "            if c1 == c2:\n",
    "                result[i, j] = min(result[i-1, j] + 1,\n",
    "                                   result[i, j-1] + 1,\n",
    "                                   result[i-1, j-1],\n",
    "                                  )\n",
    "            else:\n",
    "                result[i, j] = min(result[i-1, j] + 1,\n",
    "                                   result[i, j-1] + 1,\n",
    "                                   result[i-1, j-1] + 1,\n",
    "                                  )\n",
    "    return result[m-1, n-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_oov(word, k=10):\n",
    "    \n",
    "    lev_dist = []\n",
    "    for w2 in intersection_words:\n",
    "        lev_dist.append(levenstein_dist(word, w2))\n",
    "    min_indices = np.argpartition(lev_dist, k)[:k]\n",
    "    close_words = intersection_words[min_indices]\n",
    "    \n",
    "    neighbors, distances = knn(word, close_words, embeddings, word_id, id_word)\n",
    "    \n",
    "    if neighbors is None:\n",
    "        return None\n",
    "    \n",
    "    return close_words[np.argmin(distances)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_list = np.array(list(tag_set))\n",
    "tag_to_idx = {tag: int(idx) for (idx, tag) in enumerate(tag_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_info(prob_matrix, m, n, k=25):\n",
    "    print(\"ROW \",m)\n",
    "    for i in range(n - m):\n",
    "        print(tag_list[np.argsort(prob_matrix[m, i])[-k:]])\n",
    "        print(prob_matrix[m, i, np.argsort(prob_matrix[m, i])[-k:]])\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cyk(words, use_oov=False):\n",
    "    \"\"\"Computes the CYK matrix for a sentence\"\"\"\n",
    "    \n",
    "    n = len(words)\n",
    "    \n",
    "    cyk_matrix = np.zeros((n, n, len(tag_list), 3), dtype=\"int16\") # Backtracking Matrix\n",
    "    prob_matrix = np.zeros((n, n, len(tag_list)))\n",
    "    \n",
    "    # Unit words handling\n",
    "    for i, w in enumerate(words):\n",
    "        token_to_tag = w\n",
    "\n",
    "        if not w in lexicon:\n",
    "            if use_oov: \n",
    "                print(w + \" is an OOV\")\n",
    "            token_to_tag = handle_oov(w)\n",
    "            if use_oov:\n",
    "                if token_to_tag is None:\n",
    "                    print(\"No closest token found\\n\")\n",
    "                else:\n",
    "                    print(\"Closest token found :\", token_to_tag, \"\\n\")\n",
    "        \n",
    "        if token_to_tag is None:\n",
    "            for tag, prob in tag_freq.items():\n",
    "                idx = tag_to_idx.get(tag)\n",
    "                if not idx is None:  # avoid the case where tag appearing in lexicon but not in grammar rules\n",
    "                    prob_matrix[0, i, idx] = prob\n",
    "            \n",
    "        else:\n",
    "            max_tag, max_prob = -1, -1\n",
    "            for tag, prob in lexicon[token_to_tag].items():\n",
    "                idx = tag_to_idx.get(tag)\n",
    "                if not idx is None:\n",
    "                    prob_matrix[0, i , idx] = prob\n",
    "        \n",
    "        for parent_tag, children in binary_pcfg.items():\n",
    "            parent_tag_idx = tag_to_idx[parent_tag]\n",
    "            for child_tag, prob in children.items():\n",
    "                child_tag = child_tag.split(',')\n",
    "                if len(child_tag) > 1:\n",
    "                    continue\n",
    "                child_tag_idx = tag_to_idx[child_tag[0]]\n",
    "                if prob_matrix[0, i , child_tag_idx] > 0:\n",
    "                    prob_matrix[0, i, parent_tag_idx] = prob\n",
    "    \n",
    "    row_info(prob_matrix, 0,  n)\n",
    "    \n",
    "    # Strings of length 2 and more \n",
    "    for l in range(1, n):\n",
    "        for s in range(n - l):\n",
    "            for parent_tag, children in binary_pcfg.items():\n",
    "                for children_tags, prob in children.items():\n",
    "                    children_tags = children_tags.split(',')\n",
    "                    if len(children_tags) < 2:\n",
    "                        continue\n",
    "                    parent_tag_idx = tag_to_idx[parent_tag]\n",
    "                    left_tag_idx = tag_to_idx[children_tags[0]]\n",
    "                    right_tag_idx = tag_to_idx[children_tags[1]]\n",
    "                    prob_prod = prob\n",
    "                    for partition in range(l):\n",
    "                        prob_left = prob_matrix[partition, s, left_tag_idx]\n",
    "                        prob_right = prob_matrix[l - partition - 1, s + partition + 1, right_tag_idx]\n",
    "                    \n",
    "                        prob_split = prob_prod  * prob_left * prob_right\n",
    "                        if prob_split > prob_matrix[l, s, parent_tag_idx]:\n",
    "                            prob_matrix[l, s, parent_tag_idx] = prob_split\n",
    "                            cyk_matrix[l, s, parent_tag_idx]  = [partition, left_tag_idx, right_tag_idx]\n",
    "        \n",
    "        row_info(prob_matrix, l,  n)\n",
    "\n",
    "    result = cyk_matrix[n-1, 0, tag_to_idx['SENT']]\n",
    "    print(result[1:])\n",
    "    print(tag_list[int(result[1])])\n",
    "    print(tag_list[int(result[2])])\n",
    "    for i in reversed(range(n)):\n",
    "        tags = []\n",
    "        for j in range(n - i):\n",
    "            idx = np.argmax(prob_matrix[i, j])\n",
    "            tags.append(tag_list[idx])\n",
    "        print(tags)\n",
    "        \n",
    "    return cyk_matrix, prob_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_cyk_output(cyk, root_idx, words):\n",
    "    return parse_substring(0, len(words) - 1, root_idx, words, cyk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_substring(s, l, idx_root_tag, sentence, cyk):\n",
    "        # parse substring beginning at index s of sentence, of length l+1, and tagged as idx_root_tag\n",
    "\n",
    "        if l == 0:\n",
    "            return sentence[s]\n",
    "\n",
    "        else:  # split enabling to reach max_proba_derivation[s,l,idx_root_tag]\n",
    "            cut = int(cyk[l, s, idx_root_tag, 0])\n",
    "            idx_left_tag = int(cyk[l, s, idx_root_tag, 1])\n",
    "            idx_right_tag = int(cyk[l, s, idx_root_tag, 2])\n",
    "\n",
    "            left_tag = tag_list[idx_left_tag]\n",
    "            right_tag = tag_list[idx_right_tag]\n",
    "\n",
    "            return [[left_tag, parse_substring(s, cut, idx_left_tag, sentence, cyk)],\n",
    "                    [right_tag, parse_substring(s + cut + 1, l - cut - 1, idx_right_tag, sentence, cyk)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_parsing(parsing):    \n",
    "    if isinstance(parsing, str):\n",
    "        return parsing\n",
    "    \n",
    "    else:\n",
    "        result = \"\"\n",
    "        for p in parsing:\n",
    "            tag = p[0]\n",
    "            to_parse = p[1]\n",
    "            parsed = format_parsing(to_parse)\n",
    "            result += \"(\" + tag + \" \" + parsed + \")\" + \" \"\n",
    "        result = result[:-1]\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_artificial_tokens(s):\n",
    "    \"\"\"Remove added tags and de-telescope tags\"\"\"\n",
    "    \n",
    "    s = re.sub(r'\\(([A-Za-z\\+\\&]+[1-9]+)(\\s)(?=\\()' , '', s) # Remove numbered Tags (artificial tags)\n",
    "    s = re.sub(r' +' , ' ', s) \n",
    "    parenthesis = 0\n",
    "    i = 0\n",
    "    while i < len(s):\n",
    "        if s[i] == '(':\n",
    "            parenthesis += 1\n",
    "        elif s[i] == ')':\n",
    "            parenthesis -= 1\n",
    "        if parenthesis < 0:\n",
    "            s = ''.join([s[:i-1], s[i:]])\n",
    "            parenthesis ++ 1\n",
    "        else:\n",
    "            i += 1\n",
    "        \n",
    "    count = 0\n",
    "    for i in range(2, len(s)):\n",
    "        if i < len(s)-1 and s[i] == '&' :\n",
    "            s = s[:i] + ' (' + s[i+1:]\n",
    "            count += 1\n",
    "        elif s[i] == ')' and count > 0:\n",
    "            s = s[:i+1] + count*')' + s[i+1:]\n",
    "            count = 0\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parsed_output(sentence, use_oov=False):\n",
    "    \n",
    "    words = sentence.lower().split(' ')\n",
    "    n = len(words)\n",
    "    \n",
    "    if n > 1:\n",
    "        cyk, probs = compute_cyk(words, use_oov=use_oov)\n",
    "        root_idx = tag_to_idx['SENT']\n",
    "        if probs[n-1, 0, root_idx] == 0:\n",
    "            return None\n",
    "        parsed_output = parse_cyk_output(cyk, root_idx, words)\n",
    "    \n",
    "    else:\n",
    "        word = words[0]\n",
    "        token = word\n",
    "        if not token in lexicon:\n",
    "            token = handle_oov(word)\n",
    "        if token is None:\n",
    "            max(tag_freq, key= lambda x: x[1])\n",
    "        else:\n",
    "            tag = max(lexicon[token])\n",
    "        parsed_output = \"(\" + tag + \" \" + token + \")\"\n",
    "        \n",
    "    parsed_output = format_parsing(parsed_output)\n",
    "    parsed_output = remove_artificial_tokens(parsed_output)\n",
    "            \n",
    "    return \"( (SENT \" + parsed_output + \"))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_eg(idx):\n",
    "    print(untagged_sentences[idx])\n",
    "    print(get_parsed_output(untagged_sentences[idx], use_oov=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_entry(entry):\n",
    "    if isinstance(entry, str):\n",
    "        print(binary_pcfg[entry])\n",
    "    else:\n",
    "        for e in entry:\n",
    "            print(binary_pcfg[e])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amélioration de la sécurité\n",
      "ROW  0\n",
      "['NP773&NC' 'NP773' 'NP470&NC' 'NP54' 'NP54&NC' 'NP929' 'NP929&NC' 'NP338'\n",
      " 'NP202' 'NP338&NC' 'NP202&NC' 'NP497&NC' 'NP497' 'NP506' 'NP506&NC'\n",
      " 'NP35&NC' 'NP35' 'NP211&NC' 'NP211' 'NP453' 'NP453&NC' 'NP1' 'NP1&NC'\n",
      " 'NP&NC' 'NC']\n",
      "[3.89429764e-04 3.89429764e-04 3.89429764e-04 4.45062587e-04\n",
      " 4.45062587e-04 5.56328234e-04 5.56328234e-04 7.78859527e-04\n",
      " 7.78859527e-04 7.78859527e-04 7.78859527e-04 1.27955494e-03\n",
      " 1.27955494e-03 2.33657858e-03 2.33657858e-03 2.67037552e-03\n",
      " 2.67037552e-03 2.94853964e-03 2.94853964e-03 5.50764951e-03\n",
      " 5.50764951e-03 1.35744089e-02 1.35744089e-02 1.44033380e-01\n",
      " 1.00000000e+00]\n",
      "\n",
      "['NP3424' 'SENT8003' 'SENT8144' 'Ssub11464' 'NP1299' 'NP3382' 'SENT8376'\n",
      " 'SENT4391' 'NP3368' 'SENT8403' 'NP1412' 'NP3203' 'Ssub12000' 'NP1716'\n",
      " 'Srel12936' 'NP1399' 'SENT6735' 'SENT9457' 'Ssub11860' 'SENT5193&PONCT'\n",
      " 'NP&DET' 'P+D' 'PP&P' 'DET' 'P']\n",
      "[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 3.68158390e-06 3.19693095e-04 6.91596805e-04 2.20588235e-02\n",
      " 9.77621483e-01]\n",
      "\n",
      "['NP3282' 'NP3587' 'NP4081' 'NP&NPP' 'NP2645' 'NP3456' 'NP2925' 'NP2744'\n",
      " 'NP1644' 'NP2869' 'NP400' 'NP3029' 'NP&DET' 'NP391' 'NP3356' 'NP2891'\n",
      " 'NP2767' 'NP650' 'NP2579' 'NP508' 'NPP' 'NP960' 'NP113' 'CLO' 'DET']\n",
      "[5.56328234e-05 5.56328234e-05 5.56328234e-05 7.04936460e-05\n",
      " 1.11265647e-04 1.11265647e-04 1.11265647e-04 1.11265647e-04\n",
      " 1.11265647e-04 1.11265647e-04 1.11265647e-04 1.11265647e-04\n",
      " 1.65469545e-04 1.66898470e-04 2.22531293e-04 2.78164117e-04\n",
      " 2.78164117e-04 4.45062587e-04 6.11961057e-04 1.00139082e-03\n",
      " 1.14155251e-03 2.28094576e-03 5.28511822e-03 7.42009132e-03\n",
      " 9.91438356e-01]\n",
      "\n",
      "['NP773&NC' 'NP773' 'NP470&NC' 'NP54' 'NP54&NC' 'NP929' 'NP929&NC' 'NP338'\n",
      " 'NP202' 'NP338&NC' 'NP202&NC' 'NP497&NC' 'NP497' 'NP506' 'NP506&NC'\n",
      " 'NP35&NC' 'NP35' 'NP211&NC' 'NP211' 'NP453' 'NP453&NC' 'NP1' 'NP1&NC'\n",
      " 'NP&NC' 'NC']\n",
      "[3.89429764e-04 3.89429764e-04 3.89429764e-04 4.45062587e-04\n",
      " 4.45062587e-04 5.56328234e-04 5.56328234e-04 7.78859527e-04\n",
      " 7.78859527e-04 7.78859527e-04 7.78859527e-04 1.27955494e-03\n",
      " 1.27955494e-03 2.33657858e-03 2.33657858e-03 2.67037552e-03\n",
      " 2.67037552e-03 2.94853964e-03 2.94853964e-03 5.50764951e-03\n",
      " 5.50764951e-03 1.35744089e-02 1.35744089e-02 1.44033380e-01\n",
      " 1.00000000e+00]\n",
      "\n",
      "\n",
      "ROW  1\n",
      "['NP1095' 'NP1819' 'NP3424' 'SENT8003' 'SENT8144' 'Ssub11464' 'NP1299'\n",
      " 'NP3382' 'NP3368' 'SENT7059' 'SENT4391' 'SENT8403' 'NP1412' 'NP3203'\n",
      " 'Ssub12000' 'NP1716' 'Srel12936' 'NP1399' 'SENT6735' 'SENT9457'\n",
      " 'SENT5832' 'SENT8376' 'SENT6026' 'Ssub11860' 'SENT5193&PONCT']\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n",
      "\n",
      "['Ssub11860' 'Ssub11718' 'NP1412' 'NP3203' 'SENT8003' 'Ssub12000'\n",
      " 'Srel12936' 'NP1819' 'NP1399' 'SENT6735' 'SENT9457' 'NP1095' 'SENT5832'\n",
      " 'SENT6966' 'SENT8376' 'SENT6026' 'NP1716' 'NP3424' 'NP2217' 'NP4013'\n",
      " 'NP3274' 'NP2743' 'NP2868' 'NP' 'PP']\n",
      "[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 6.82723023e-11 6.82723023e-11\n",
      " 6.82723023e-11 2.73089209e-10 2.73089209e-10 4.20272135e-07\n",
      " 3.91814628e-04]\n",
      "\n",
      "['NP3368' 'SENT9457' 'SENT4391' 'NP3203' 'NP1399' 'Srel12936' 'NP1716'\n",
      " 'Ssub12000' 'Ssub11860' 'NP1412' 'SENT5193&PONCT' 'SENT8403' 'NP2927'\n",
      " 'NP3578' 'NP3517' 'NP1949' 'NP1840' 'NP1637' 'NP1771' 'NP239' 'NP928'\n",
      " 'NP337' 'NP496' 'NP505' 'NP']\n",
      "[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 3.53311762e-12 3.53311762e-12 3.53311762e-12 3.06851265e-09\n",
      " 3.06851265e-09 3.06851265e-09 1.22740506e-08 1.10466456e-07\n",
      " 3.06851265e-07 6.01428480e-07 1.62324319e-06 5.41285632e-06\n",
      " 1.50853068e-01]\n",
      "\n",
      "\n",
      "ROW  2\n",
      "['NP1819' 'NP3424' 'SENT8003' 'SENT8144' 'Ssub11464' 'NP1299' 'NP3382'\n",
      " 'NP3368' 'Ssub11718' 'SENT4391' 'SENT8403' 'NP1412' 'NP3203' 'Ssub12000'\n",
      " 'NP1716' 'Srel12936' 'NP1399' 'SENT6735' 'SENT9457' 'SENT5832' 'SENT8376'\n",
      " 'Ssub11860' 'SENT5193&PONCT' 'NP3273' 'NP']\n",
      "[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 3.79818093e-15\n",
      " 2.14271922e-05]\n",
      "\n",
      "['NP3382' 'NP3368' 'SENT6026' 'Ssub11860' 'SENT4391' 'NP1412' 'NP3203'\n",
      " 'Ssub12000' 'NP1716' 'SENT5832' 'SENT9457' 'SENT6735' 'Srel12936'\n",
      " 'SENT8403' 'NP1399' 'SENT9337' 'NP1839' 'NP46' 'NP1905' 'NP1458' 'NP1770'\n",
      " 'NP927' 'NP336' 'NP' 'PP']\n",
      "[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 4.37610207e-14\n",
      " 1.66889785e-13 1.21267060e-12 1.21267060e-12 1.21267060e-12\n",
      " 1.33511828e-12 1.66889785e-10 4.57945571e-10 1.85126076e-07\n",
      " 1.11841224e-01]\n",
      "\n",
      "\n",
      "ROW  3\n",
      "['NP1095' 'NP1819' 'NP3424' 'SENT8003' 'SENT8144' 'Ssub11464' 'NP1299'\n",
      " 'NP3382' 'NP3368' 'SENT4391' 'Ssub11860' 'SENT8403' 'NP1412' 'NP3203'\n",
      " 'Ssub12000' 'NP1716' 'Srel12936' 'NP1399' 'SENT6735' 'SENT9457'\n",
      " 'SENT8376' 'SENT5193&PONCT' 'NP1904' 'SENT9337' 'NP']\n",
      "[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 6.74642892e-17 2.23111580e-12\n",
      " 6.11626833e-03]\n",
      "\n",
      "\n",
      "[0 0]\n",
      "SENT7889\n",
      "SENT7889\n",
      "['NP']\n",
      "['NP', 'PP']\n",
      "['SENT7889', 'PP', 'NP']\n",
      "['NC', 'P', 'DET', 'NC']\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print_eg(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
